{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 2 - Credit Default\n",
    "**Alex Cattie, Mike Szemenyei, Kerry Clarke**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding & Exploration\n",
    "### **Business Understanding - Framing the Analytical Question**\n",
    "- How accurately are we able to predict that a customer is going to default?\n",
    "\n",
    "Full Notebook: [Business Understanding](EDA/Business_Understanding.ipynb)\n",
    "\n",
    "### **Data Dictionary**\n",
    "| Column Name  | Contents |\n",
    "| ------------- | ------------- |\n",
    "| **X1** | Amount of credit that an individual or family was given |\n",
    "| **X2** | Gender (1=M; 2=F) | \n",
    "| **X3** | Education level (1 = graduate school; 2 = university; 3 = high school; 4 = others; 5, 6, 0 = unknown) | \n",
    "| **X4** | Martial Status (1=married; 2=single; 3=others, 0=unknown) | \n",
    "| **X5** | Age (year) | \n",
    "| **X6** | Repayment status in September 2005 | \n",
    "| **X7** | Repayment status in August 2005 | \n",
    "| **X8** | Repayment status in July 2005 | \n",
    "| **X9** | Repayment status in June 2005 | \n",
    "| **X10** | Repayment status in May 2005 | \n",
    "| **X11** | Repayment status in April 2005 | \n",
    "| **X12** | Amount of bill statement in September 2005 | \n",
    "| **X13** | Amount of bill statement in August 2005 | \n",
    "| **X14** | Amount of bill statement in July 2005 | \n",
    "| **X15** | Amount of bill statement in June 2005 | \n",
    "| **X16** | Amount of bill statement in May 2005 | \n",
    "| **X17** | Amount of bill statement in April 2005 | \n",
    "| **X18** | Amount paid in September 2005 | \n",
    "| **X19** | Amount paid in August 2005 | \n",
    "| **X20** | Amount paid in July 2005 | \n",
    "| **X21** | Amount paid in June 2005 | \n",
    "| **X22** | Amount paid in May 2005 | \n",
    "| **X23** | Amount paid in April 2005 | \n",
    "| **Y** | Target - Default (Yes=1; No=0) | \n",
    "\n",
    "Full Notebook: [Data Dictionary](EDA/Data_Dictionary.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Understanding - EDA**\n",
    "\n",
    "\n",
    "![Gender Counts](EDA/images/gender.PNG)\n",
    "\n",
    "![Education Level](EDA/images/education.PNG)\n",
    "\n",
    "![Marital Status](EDA/images/maritalstatus.png)\n",
    "\n",
    "![Age Histogram](EDA/images/age.PNG)\n",
    "\n",
    "![Target](EDA/images/y_imbalanced.png)\n",
    "\n",
    "![X1 and X5](EDA/images/x1x5.PNG)\n",
    "\n",
    "![X12 and X13](EDA/images/x12x13.PNG)\n",
    "\n",
    "![X14 and X15](EDA/images/x14x15.PNG)\n",
    "\n",
    "![X16 and X17](EDA/images/x16x17.PNG)  \n",
    "\n",
    "\n",
    "\n",
    "Full Notebook: [Data Understanding](EDA/Data_Understanding_EDA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Modeling with Raw Data**\n",
    "- **Decision Tree Results**  \n",
    "![DT Results](Raw/images/dt_raw_results.PNG)    \n",
    "\n",
    "- **Logistic Regression Results**  \n",
    "![Log Reg Results](Raw/images/logreg_raw_results.PNG)    \n",
    "\n",
    "- We introduced resampling in these models to address the imbalanced nature of the Target that we discovered during EDA.  \n",
    "\n",
    "Notebooks:\n",
    "- [Decision Tree & Logistic Regression](Raw/decision_tree_logreg.ipynb)\n",
    "- [TPOT](Raw/TPOT_raw.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Preprocessing \n",
    "### **Data Preprocessing**\n",
    "**Pipeline 1 - Assumes no Normal Distribution**\n",
    "- IQR\n",
    "    - Found the IQR for each column of data, created a function to go through each column and determine whether the value is an outlier\n",
    "    - If the value is an outlier, determine whether it is a high or low outlier and if so, replace the initial number with the low or high IQR outlier value\n",
    "- Min/Max \n",
    "    - Use the updated dataset with outliers removed to standardize the data\n",
    "    - Utilized skearnâ€™s preprocessing package (MinMaxScaler()) to find the min and max values in each column\n",
    "    - The Fit_transform() method then scaled each column with a value from 0 to 1 based on the min and max from each column\n",
    "- Skew\n",
    "    - Analyze the overall flow of the data to account for any skewness \n",
    "    - Take each individual variable and apply the sqrt or cbrt function to fix the overall transformation of the dataset  \n",
    "  \n",
    "**Pipeline 2 - Assumes Normal Distribution**  \n",
    "\n",
    "### **Balancing the Target**\n",
    "- Critical Step to take before modeling\n",
    "- With a balanced target we can be more confident that our modeling results are xx% better than random guessing\n",
    "- With an imbalanced target we are less confident because an imbalanced target throws off the models \n",
    "- We used the **imblearn package** to help balance the target (see code below)  \n",
    "![Code for Balancing the Target](images/balancing_target.PNG)\n",
    "\n",
    "### **Cross Validation vs. Train_Test_Split** \n",
    "- Using the train_test_split, we only have 1 static split of the data for training and testing\n",
    "- Cross Validation is a a better was to split the data for modeling because it gives us k folds for training and testing\n",
    "    - Cross Validation also reduces the \"leaking\" of the testing data into the training data\n",
    "    - We used the **Stratified KFolds** package to do Cross Validation \n",
    "        - Code will be shown below in the modeling section \n",
    "\n",
    "### **Feature Engineering**\n",
    "- Based on our discoveries from EDA we decided to do some feature engineering\n",
    "\n",
    "- **X3-X5: Education, Marital Status, Age**\n",
    "    - We created bins for these variables as we saw that they contained values that were not defined in the data dictionary provided\n",
    "    - By binning these variables we made them easier to work with and we have accounted for all the unknown values in the features  \n",
    "    \n",
    "![Creating Bins Code Example](images/edu_bins.PNG)\n",
    "![Education Bins](images/edu_bins_pic.PNG)\n",
    "![Marital Status Bins](images/marital_bins_pic.PNG)\n",
    "![Age Bins](images/age_bins_pic.PNG)\n",
    "  \n",
    "  \n",
    "- **X6-X11: History of Past Payment**\n",
    "    - Only concerned about it the customer paid on time or was late \n",
    "      - Binning makes the feature binary - much easier to work with\n",
    "           - 1 = on time\n",
    "           - 2 = delayed/late  \n",
    "           \n",
    "![Binning X6-X11](images/x6bins.PNG)  \n",
    "  \n",
    "  \n",
    "- **X12-X17: Anount of Bill Statement**\n",
    "     - **3 new variables will be created for these features**\n",
    "          - 1: The first will be the absolute value of the original payment amount\n",
    "          - 2: The second will address the positivity or negativity of the original payment amount (binary variable - will account for the negative values in the original that were removed in the first new features - maintains integrity of original data)\n",
    "          - 3: The third will address the time series nature of these features and will be the monthly difference from when the data was collected to the time when the payment was made  \n",
    "  \n",
    "  \n",
    "- **X18-X23: Amount of Previous Payment**\n",
    "     - Only concerned if they paid or not that month - not the specific amount \n",
    "          - 0 = no payment\n",
    "          - 1 = payment\n",
    "\n",
    "\n",
    "Notebooks:\n",
    "- [Pipeline 1](pipeline_1/p1_full.ipynb)\n",
    "- [Pipeline 2](pipeline_2/p2_full.ipynb)\n",
    "- [Feature Engineering](comp2_initial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Modeling \n",
    "- The majority of out time was spent working with various models from basic models like Decision Tree and Logistic Regression to much more advanced models like XGBoost and Random Forest  \n",
    "- These models have no parameters specified other than the random state and n_estimators for XGBoost & Random Forest\n",
    "\n",
    "### **All Models:**\n",
    "- Decistion Tree\n",
    "- Logistic Regression\n",
    "- KNeighborsClassifier\n",
    "- AdaBoost\n",
    "- XGBoost\n",
    "- Random Forest\n",
    "\n",
    "![P1 All Models Results](images/p1cv_all_results.PNG)  \n",
    "![P1 Random Forest](images/p1_rf.PNG)\n",
    "\n",
    "\n",
    "### **Selected Models With Parameters**\n",
    "- Decision Tree\n",
    "- XGBoost\n",
    "- Random Forest  \n",
    "    \n",
    "![P1 Selected Models Results](images/p1cv_selected_results.PNG)\n",
    "![P1 Random Forest Parameters](images/p1_params_fr.PNG)  \n",
    "\n",
    "\n",
    "### **Consolidated Best Results**\n",
    "- **Pipeline 1**\n",
    "    - Decision Tree: F1=0.74115; AUC=0.81325\n",
    "    - XGBoost: F1=0.77072, AUC=0.84540\n",
    "    - Random Forest: F1=0.76286, AUC=0.85512\n",
    "- **Pipeline 2**\n",
    "    - Decision Tree: F1=0.74852, AUC=0.81803\n",
    "    - XGBoost: F1=0.76949, AUC=0.84001\n",
    "    - Random Forest: F1=0.76941, AUC=0.85767\n",
    "\n",
    "\n",
    "Notebooks:\n",
    "- [Pipeline 1 Models](pipeline_1/p1_CVmodels.ipynb)\n",
    "- [Pipeline 1 Random Forest](p1_Random_Forest.ipynb)\n",
    "- [Pipeline 2 Models](pipeline_2/p2_CVmodels.ipynb)\n",
    "- [Pipeline 2 Random Forest](p2_Random_Forest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4  - Explanation of Best Model \n",
    "### **Best Model: XGBoost - Pipeline 1**\n",
    "- Original Results\n",
    "    - **F1:** 0.65837\n",
    "    - **AUC:** 0.74466  \n",
    "    \n",
    "\n",
    "- \"Tweaking\" of the model's hyperparameters (Definitions taken from week 10 XGBoost classwork notebook)\n",
    "    - **learning_rate:** step size shrinkage used to prevent overfitting. Range is `[0,1]`\n",
    "    - **max_depth:** determines how deeply each tree is allowed to grow during any boosting round\n",
    "    - **colsample_bytree:** percentage of features used per tree. High value can lead to overfitting\n",
    "    - **n_estimators:** number of trees you want to build - the more trees you build, the longer the training will be\n",
    "    - **random_state:** ensures that results can be reproduced later on \n",
    "    \n",
    "  \n",
    "- Fine Tuned Results\n",
    "    - **F1:** 0.77072\n",
    "    - **AUC:** 0.84540\n",
    "\n",
    "\n",
    "- Why it may have been the best model\n",
    "    - Explain  \n",
    "  \n",
    "\n",
    "- While the .84 AUC is a bit high and may be suspect to overfitting we went with this model because it had a higher F1 score than the other models. Also, XGBoost showed the highest margin of improvement from the Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - TPOT\n",
    "- TPOT does preprocessing and modeling automatically \n",
    "- TPOT also optimizes **performance** rather than **reproducibility**\n",
    "- It's important to know how to balance the two aspects\n",
    "    - On the one side we can explain the process start to finish but we may sacrifice performance and risk overfitting \n",
    "    - On the other with TPOT we cannot explain the preprocessing or the modeling but we tend to get better results\n",
    "- According to TPOT, **Logistic Regression** is the best performing model\n",
    "- Notesbooks:\n",
    "    - [Pipeline 1 TPOT](pipeline_1/p1_TPOT.ipynb)\n",
    "    - [Pipeline 2 TPOT](pipeline_2/p2_TPOT.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Original Question: How accurately are we able to predict that a customer is going to default?\n",
    "\n",
    "### Our Results: F1: 77%, AUC: 84% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

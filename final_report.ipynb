{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 2 - Credit Default\n",
    "**Alex Cattie, Mike Szemenyei, Kerry Clarke**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding & Exploration\n",
    "### **Business Understanding - Framing the Analytical Question**\n",
    "#### - Using existing customer information, how accurately are we able to predict that a customer is going to default?\n",
    "\n",
    "Full Notebook: [Business Understanding](EDA/Business_Understanding.ipynb)\n",
    "\n",
    "### **Data Dictionary**\n",
    "| Column Name  | Contents |\n",
    "| ------------- | ------------- |\n",
    "| **X1** | Amount of credit that an individual or family was given |\n",
    "| **X2** | Gender (1=M; 2=F) | \n",
    "| **X3** | Education level (1 = graduate school; 2 = university; 3 = high school; 4 = others; 5, 6, 0 = unknown) | \n",
    "| **X4** | Martial Status (1=married; 2=single; 3=others, 0=unknown) | \n",
    "| **X5** | Age (year) | \n",
    "| **X6** | Repayment status in September 2005 | \n",
    "| **X7** | Repayment status in August 2005 | \n",
    "| **X8** | Repayment status in July 2005 | \n",
    "| **X9** | Repayment status in June 2005 | \n",
    "| **X10** | Repayment status in May 2005 | \n",
    "| **X11** | Repayment status in April 2005 | \n",
    "| **X12** | Amount of bill statement in September 2005 | \n",
    "| **X13** | Amount of bill statement in August 2005 | \n",
    "| **X14** | Amount of bill statement in July 2005 | \n",
    "| **X15** | Amount of bill statement in June 2005 | \n",
    "| **X16** | Amount of bill statement in May 2005 | \n",
    "| **X17** | Amount of bill statement in April 2005 | \n",
    "| **X18** | Amount paid in September 2005 | \n",
    "| **X19** | Amount paid in August 2005 | \n",
    "| **X20** | Amount paid in July 2005 | \n",
    "| **X21** | Amount paid in June 2005 | \n",
    "| **X22** | Amount paid in May 2005 | \n",
    "| **X23** | Amount paid in April 2005 | \n",
    "| **Y** | Target - Default (Yes=1; No=0) | \n",
    "\n",
    "Full Notebook: [Data Dictionary](EDA/Data_Dictionary.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Understanding - EDA**\n",
    "\n",
    "- When we first initially started the competition we did some basic EDA in order to gain a better understanding of the dataset and the characteristics of the variables. This started with things as simple as printing out the head and column names of the dataset all the way down to make a correlation heatmap to see if any of the variables were closely related. We also checked to see how many rows there were as well as using the describe function in order to gain look at some more advanced statistics. We observed that there were 25 rows and 30,000 entries in each row which is what we expected. Next, some bar charts were created in order to see the distribution between the categorical variables. We looked at gender distribution, marital status, education level, and if individuals defaulted or not. For the eight nominal value columns we created histograms to show their distributions. The last step in EDA that we performed was creating a correlation heatmap of all the variables. This helped us to get a good idea of how the variables compared to one another. We felt that this amount of EDA gave us a good initial handle of the dataset before getting started on the competition.  \n",
    "\n",
    "\n",
    "![Gender Counts](EDA/images/gender.PNG)\n",
    "\n",
    "![Education Level](EDA/images/education.PNG)\n",
    "\n",
    "![Marital Status](EDA/images/maritalstatus.png)\n",
    "\n",
    "![Age Histogram](EDA/images/age.PNG)\n",
    "\n",
    "![Target](EDA/images/y_imbalanced.png)\n",
    "\n",
    "![X1 and X5](EDA/images/x1x5.PNG)\n",
    "\n",
    "![X12 and X13](EDA/images/x12x13.PNG)\n",
    "\n",
    "![X14 and X15](EDA/images/x14x15.PNG)\n",
    "\n",
    "![X16 and X17](EDA/images/x16x17.PNG)  \n",
    "\n",
    "\n",
    "\n",
    "Full Notebook: [Data Understanding](EDA/Data_Understanding_EDA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Modeling with Raw Data**\n",
    "- **Decision Tree Results**  \n",
    "![DT Results](Raw/images/dt_raw_results.PNG)    \n",
    "\n",
    "- **Logistic Regression Results**  \n",
    "![Log Reg Results](Raw/images/logreg_raw_results.PNG)    \n",
    "\n",
    "- We introduced resampling in these models to address the imbalanced nature of the Target that we discovered during EDA\n",
    "- Also ran a TPOT model with the raw data. The best model identified with the raw data was Decision Tree\n",
    "\n",
    "Notebooks:\n",
    "- [Decision Tree & Logistic Regression](Raw/decision_tree_logreg.ipynb)\n",
    "- [TPOT](Raw/TPOT_raw.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Preprocessing \n",
    "### **Data Preprocessing**\n",
    "**Pipeline 1 - Assumes no Normal Distribution**\n",
    "- IQR\n",
    "    - Found the IQR for each column of data, created a function to go through each column and determine whether the value is an outlier\n",
    "    - If the value is an outlier, determined whether it is a high or low outlier and if so, replace the initial number with the low or high IQR outlier value\n",
    "- Min/Max \n",
    "    - Used the updated dataset with outliers removed to standardize the data\n",
    "    - Utilized skearn’s preprocessing package (MinMaxScaler()) to find the min and max values in each column\n",
    "    - The Fit_transform() method then scaled each column with a value from 0 to 1 based on the min and max from each column\n",
    "- Skew\n",
    "    - Analyzed the overall flow of the data to account for any skewness \n",
    "    - Took each individual variable and applied the sqrt or cbrt function to fix the overall transformation of the dataset  \n",
    "  \n",
    "**Pipeline 2 - Assumes Normal Distribution** \n",
    "- Skew\n",
    "    - Analyzed the overall flow of the data to account for any skewness before accounting for any outliers\n",
    "    - Took each individual variable and applied the sqrt or cbrt function to fix the overall transformation of the dataset\n",
    "- 3 Standard Deviations\n",
    "    - Found the upper and lower bounds for each column: 3 standard deviations from the mean of each column\n",
    "    - Used a for loop to go through each column and determine whether a data value is an outlier and if so, replace it with the lower or upper bound\n",
    "- Z-Score \n",
    "    - Used the updated dataset with outliers removed\n",
    "    - Utilized skearn’s preprocessing package (StandardScaler()) to calculate z-scores for each data value\n",
    "\n",
    "\n",
    "### **Balancing the Target**\n",
    "- Critical Step to take before modeling\n",
    "- With a balanced target we can be more confident that our modeling results are xx% better than random guessing\n",
    "- With an imbalanced target we are less confident because an imbalanced target throws off the models \n",
    "- We used the **imblearn package** to help balance the target (see code below)  \n",
    "![Code for Balancing the Target](images/balancing_target.PNG)\n",
    "\n",
    "### **Cross Validation vs. Train_Test_Split** \n",
    "- Using the train_test_split, we only have 1 static split of the data for training and testing\n",
    "    - We built most of our models using train_test_split first and then using cross validation\n",
    "    - In this presentation we only highlight the models using Cross Validation\n",
    "- Cross Validation is a a better was to split the data for modeling because it gives us k folds for training and testing\n",
    "    - We used the **Stratified KFolds** package to do Cross Validation \n",
    "        - Code will be shown below in the modeling section \n",
    "\n",
    "### **Feature Engineering**\n",
    "- Based on our discoveries from EDA we decided to do some feature engineering\n",
    "\n",
    "- **X3-X5: Education, Marital Status, Age**\n",
    "    - We created bins for these variables as we saw that they contained values that were not defined in the data dictionary provided\n",
    "    - By binning these variables we made them easier to work with and we have accounted for all the unknown values in the features  \n",
    "    \n",
    "![Creating Bins Code Example](images/edu_bins.PNG)\n",
    "![Education Bins](images/edu_bins_pic.PNG)\n",
    "![Marital Status Bins](images/marital_bins_pic.PNG)\n",
    "![Age Bins](images/age_bins_pic.PNG)\n",
    "  \n",
    "  \n",
    "- **X6-X11: History of Past Payment**\n",
    "    - Only concerned about it the customer paid on time or was late \n",
    "      - Binning makes the feature binary - much easier to work with\n",
    "           - 1 = on time\n",
    "           - 2 = delayed/late  \n",
    "           \n",
    "![Binning X6-X11](images/x6bins.PNG)  \n",
    "  \n",
    "  \n",
    "- **X12-X17: Anount of Bill Statement**\n",
    "     - **3 new variables will be created for these features**\n",
    "          - 1: The first will be the absolute value of the original payment amount\n",
    "          - 2: The second will address the positivity or negativity of the original payment amount (binary variable - will account for the negative values in the original that were removed in the first new features - maintains integrity of original data)\n",
    "          - 3: The third will address the time series nature of these features and will be the monthly difference from when the data was collected to the time when the payment was made  \n",
    "  \n",
    "  \n",
    "- **X18-X23: Amount of Previous Payment**\n",
    "     - Only concerned if they paid or not that month - not the specific amount \n",
    "          - 0 = no payment\n",
    "          - 1 = payment\n",
    "\n",
    "\n",
    "\n",
    "Notebooks:\n",
    "- [Pipeline 1](pipeline_1/p1_full.ipynb)\n",
    "- [Pipeline 2](pipeline_2/p2_full.ipynb)\n",
    "- [Feature Engineering](comp2_initial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Modeling \n",
    "- The majority of out time was spent working with various models from basic models like Decision Tree and Logistic Regression to much more advanced models like XGBoost and Random Forest  \n",
    "- These models have no parameters specified other than the random state and n_estimators for XGBoost & Random Forest\n",
    "- Below screenshot are only of Pipeline 1. Pipeline 2 work is exactly the same and results can be found in the notebook linked below\n",
    "- Random State for all models was 2019; XGBoost base n_esimators=150; Random Forest base n_estimators=100\n",
    "- The same hyperparamaters and parameters were used for both Pipeline 1 and Pipeline 2\n",
    "\n",
    "### **All Models:**\n",
    "- Decistion Tree\n",
    "- Logistic Regression\n",
    "- KNeighborsClassifier\n",
    "- AdaBoost\n",
    "- XGBoost\n",
    "- Random Forest\n",
    "\n",
    "![P1 All Models Results](images/p1cv_USE.PNG)  \n",
    "![P1 Random Forest](images/random_forest_base_USE.PNG)  \n",
    "\n",
    "- The results of the models were mediocre and the ones that seemed to perform well are most likely overfitted. \n",
    "\n",
    "\n",
    "### **Selected Models With Parameters**\n",
    "- **Decision Tree**\n",
    "    - Random State \n",
    "        - Utilized the random_state parameter because of running the program numerous times\n",
    "        - If you don’t specify a random_state, every time the program is run, a different random_state could be used and could result in unpredictable and unreliable results (random_state=2019)\n",
    "    - Max Depth\n",
    "        - Utilized the max_depth parameter in order to limit the number of decisions a tree has to decide on each time a node is split to control the number of possible solutions and amount of overall error\n",
    "        - Tweaked to 15\n",
    "    - Max Features\n",
    "         - Limits the number of features that the tree can use for each split in the tree\n",
    "         - Tweaked to 10\n",
    "    - Criterion parameter\n",
    "        - The default for this parameter is the gini index, which is the best criterion for making sure that there is little likelihood of misclassification\n",
    "        - The other criterion option is entropy, but we chose to keep the default because this is a classification problem\n",
    "    - Splitter parameter\n",
    "        - The default for this parameter is “best”, which means that what the tree splits on a node, it will choose the most relevant feature at that time to split on\n",
    "        - If we had chosen “random” as the splitter parameter, there would be more of a chance to be forced to go more deeply into the tree and introduce more error and  less precision\n",
    "\n",
    "        \n",
    "\n",
    "- **XGBoost** (Definitions taken from XGBoost classwork notebook in week 10) \n",
    "    - Learning_rate\n",
    "        - Step size shrinkage used to prevent overfitting. Range is `[0,1]`\n",
    "        - Tweaked to 0.5\n",
    "    - Max_depth\n",
    "        - Determines how deeply each tree is allowed to grow during any boosting round\n",
    "        - Tweaked to 4\n",
    "    - Colsample_bytree\n",
    "        - Percentage of features used per tree. High value can lead to overfitting\n",
    "        - Tweaked to 0.5\n",
    "    - N_estimators\n",
    "        - Number of trees you want to build - the more trees you build, the longer the training will be\n",
    "        - Tweaked to 150\n",
    "    - Random_state\n",
    "        - Ensures that results can be reproduced later on. Random_state=2019\n",
    "        \n",
    "    \n",
    "- **Random Forest**\n",
    "    - Max Depth\n",
    "        - Defined as the depth of each tree in the forest\n",
    "        - By limiting to 13 we increase the speed of the model \n",
    "    - Max Features\n",
    "        - Defined as the number of features to consider when looking for the best split\n",
    "        - By limiting to 8 we have told the model it may only consider 8 features before it must move on. This increases the speed of the model\n",
    "    - Min Sample Spilt\n",
    "        - Defined as the minimum number of samples required to split an internal node\n",
    "        - By specifiying to 10, we have told the model it must consider 10 separate samples before moving to the next spilt\n",
    "    - N_Estimators\n",
    "        - Limited to 100 so to improve the run time of the model (would run and find a solution faster)\n",
    "    - Random State\n",
    "        - Specified as 2019 to ensure that results could be reproduced\n",
    "    - Used this article for help on understanding Random Forest parameters [Random Forest Article](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d)\n",
    "    \n",
    "![P1 Selected Models Results](images/p1cv_selected_USE.PNG)\n",
    "![P1 Random Forest Parameters](images/random_forest_tweaked_USE.PNG)  \n",
    "\n",
    "\n",
    "### **Consolidated Best Results**\n",
    "- **Pipeline 1**\n",
    "    - Decision Tree: F1=0.74115; AUC=0.81325\n",
    "    - XGBoost: F1=0.75409, AUC=0.82760\n",
    "    - Random Forest: F1=0.76286, AUC=0.85512\n",
    "- **Pipeline 2**\n",
    "    - Decision Tree: F1=0.74852, AUC=0.81803\n",
    "    - XGBoost: F1=0.75599, AUC=0.82677\n",
    "    - Random Forest: F1=0.76941, AUC=0.85767\n",
    "\n",
    "\n",
    "Notebooks:\n",
    "- [Pipeline 1 Models](pipeline_1/p1_CVmodels.ipynb)\n",
    "- [Pipeline 1 Random Forest](p1_Random_Forest.ipynb)\n",
    "- [Pipeline 2 Models](pipeline_2/p2_CVmodels.ipynb)\n",
    "- [Pipeline 2 Random Forest](p2_Random_Forest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4  - Explanation of Best Model \n",
    "### **Best Model: XGBoost - Pipeline 1**\n",
    "- **Original Results**\n",
    "    - **F1:** 0.65837\n",
    "    - **AUC:** 0.74466  \n",
    "    \n",
    "\n",
    "- **\"Tweaking\" of the model's hyperparameters**\n",
    "    - **Col_Sample_Bytree**\n",
    "        - The first parameter that we tried tuning was col_sample_bytree which is choosing the percentage of the features that we want to be involved in the trees each time the model. We were well aware that raising this to high could potentially cause over-fitting. We started with it at 0.5 and changed it in .1 increments all the way up to 1.0 and honestly did not see our score change that much. Because of this we decided to keep it at 0.5 in order to ensure that we do not overfit the model\n",
    "    - **Learning Rate**\n",
    "        - The second parameter that we tuned was learning rate. We initially started with it at 0.5. We adjusted it in 0.1 increments between .4 and 1.0 and did not find a noticeable difference. We ended up choosing 0.5 to keep for our model\n",
    "    - **Max Depth**\n",
    "        - Next, we tuned the max_depth, which is how deep each of the trees are allowed to go when the model is run. This was one of the two parameters that we found could most impact the model. When we increased the depth the score increased greatly. While it was great to get high scores we were concerned that the model might become over-fitted by using a higher max_depth. We choose to put max_depth=4\n",
    "    - **N_Estimators**\n",
    "        - The last parameter that we tuned was the n_estimators, which was the number of trees that we wanted the model to make. We realized that the more trees that we tried to create that the longer the model would take to run. We initially started using a 100 trees and increased the number all the way up to 250. Each time we increased the number of estimators we received a bit of a higher score; however, in order to not have the model take a very long time to run we decided that the best amount of estimators was 150 \n",
    "  \n",
    "\n",
    "- **Fine Tuned Results**\n",
    "    - **F1:** 0.75409\n",
    "    - **AUC:** 0.82760\n",
    "\n",
    "\n",
    "- **Feature Importance**\n",
    "![XGBoost Feature Importance](images/XGB_featureimport.PNG)\n",
    "- The most important features were the absolute values of the bill statement amouunts and the original amount of credit given to the individual or family received. These features can be used to help identify the customers that may be more likely to default on their payments\n",
    "\n",
    "- **We chose XGBoost has the best model because it showed the highest margin of improvement from the Baseline model and we felt that the Random Forest was too prone to over-fitting even though the results were higher.** \n",
    "\n",
    "\n",
    "- Notebooks:\n",
    "    - [Pipeline 1 XGBoost](p1_XGBoost.ipynb)\n",
    "    - [Pipeline 2 XGBoost](p2_XGBoost.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - TPOT\n",
    "- TPOT does preprocessing and modeling automatically \n",
    "- TPOT also optimizes **performance** rather than **reproducibility**\n",
    "- It's important to know how to balance the two aspects\n",
    "    - On the one side we can explain the process start to finish but we may sacrifice performance and risk overfitting \n",
    "    - On the other with TPOT we cannot explain the preprocessing or the modeling but we tend to get better results\n",
    "- According to TPOT, **Logistic Regression** is the best performing model\n",
    "- Notesbooks:\n",
    "    - [Pipeline 1 TPOT](pipeline_1/p1_TPOT.ipynb)\n",
    "    - [Pipeline 2 TPOT](pipeline_2/p2_TPOT.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Original Question: How accurately are we able to predict that a customer is going to default?\n",
    "\n",
    "### Our Results: F1: 0.75, AUC: 0.82 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

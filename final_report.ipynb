{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 2 - Credit Default\n",
    "\n",
    "**Alex Cattie, Mike Szemenyei, Kerry Clarke**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Understanding & Exploration\n",
    "\n",
    "### **Business Understanding - Framing the Analytical Question**\n",
    "- How accurately are we able to predict that a customer is going to default?\n",
    "\n",
    "Full Notebook: [Business Understanding](EDA/Business_Understanding.ipynb)\n",
    "\n",
    "### **Data Dictionary**\n",
    "| Column Name  | Contents |\n",
    "| ------------- | ------------- |\n",
    "| **X1** | Amount of credit that an individual or family was given |\n",
    "| **X2** | Gender (1=M; 2=F) | \n",
    "| **X3** | Education level (1 = graduate school; 2 = university; 3 = high school; 4 = others) | \n",
    "| **X4** | Martial Status (1=married; 2=single; 3=others) | \n",
    "| **X5** | Age (year) | \n",
    "| **X6** | Repayment status in September 2005 | \n",
    "| **X7** | Repayment status in August 2005 | \n",
    "| **X8** | Repayment status in July 2005 | \n",
    "| **X9** | Repayment status in June 2005 | \n",
    "| **X10** | Repayment status in May 2005 | \n",
    "| **X11** | Repayment status in April 2005 | \n",
    "| **X12** | Amount of bill statement in September 2005 | \n",
    "| **X13** | Amount of bill statement in August 2005 | \n",
    "| **X14** | Amount of bill statement in July 2005 | \n",
    "| **X15** | Amount of bill statement in June 2005 | \n",
    "| **X16** | Amount of bill statement in May 2005 | \n",
    "| **X17** | Amount of bill statement in April 2005 | \n",
    "| **X18** | Amount paid in September 2005 | \n",
    "| **X19** | Amount paid in August 2005 | \n",
    "| **X20** | Amount paid in July 2005 | \n",
    "| **X21** | Amount paid in June 2005 | \n",
    "| **X22** | Amount paid in May 2005 | \n",
    "| **X23** | Amount paid in April 2005 | \n",
    "| **Y** | Target - Default (Yes=1; No=0) | \n",
    "\n",
    "Full Notebook: [Data Dictionary](EDA/Data_Dictionary.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Understanding - EDA**\n",
    "\n",
    "\n",
    "![Gender Counts](EDA/gender.PNG)\n",
    "\n",
    "![Education Level](EDA/education.PNG)\n",
    "\n",
    "![Marital Status](EDA/maritalstatus.png)\n",
    "\n",
    "![Age Histogram](EDA/age.PNG)\n",
    "\n",
    "![Target](EDA/y_imbalanced.png)\n",
    "\n",
    "![X1 and X5](EDA/x1x5.PNG)\n",
    "\n",
    "![X12 and X13](EDA/x12x13.PNG)\n",
    "\n",
    "![X14 and X15](EDA/x14x15.PNG)\n",
    "\n",
    "![X16 and X17](EDA/x16x17.PNG)  \n",
    "\n",
    "\n",
    "\n",
    "Full Notebook: [Data Understanding](EDA/Data_Understanding_EDA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Modeling with Raw Data**\n",
    "- **Decision Tree Results**  \n",
    "![DT Results](Raw/dt_raw_results.PNG)    \n",
    "\n",
    "- **Logistic Regression Results**  \n",
    "![Log Reg Results](Raw/logreg_raw_results.PNG)    \n",
    "\n",
    "- We introduced resampling in these models to address the imbalanced nature of the Target that we discovered during EDA.  \n",
    "\n",
    "Notebooks:\n",
    "- Full Notebook: [Decision Tree & Logistic Regression](Raw/decision_tree_logreg.ipynb)\n",
    "- TPOT Notebook: [TPOT](Raw/TPOT_raw.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**\n",
    "**Pipeline 1**\n",
    "- IQR\n",
    "    - Found the IQR for each column of data, created a function to go through each column and determine whether the value is an outlier\n",
    "    - If the value is an outlier, determine whether it is a high or low outlier and if so, replace the initial number with the low or high IQR outlier value\n",
    "- Min/Max \n",
    "    - Use the updated dataset with outliers removed to standardize the data\n",
    "    - Utilized skearnâ€™s preprocessing package (MinMaxScaler()) to find the min and max values in each column\n",
    "    - The Fit_transform() method then scaled each column with a value from 0 to 1 based on the min and max from each column\n",
    "- Skew\n",
    "    - Analyze the overall flow of the data to account for any skewness \n",
    "    - Take each individual variable and apply the sqrt or cbrt function to fix the overall transformation of the dataset  \n",
    "  \n",
    "**Pipeline 2**  \n",
    "\n",
    "**Balancing the Target**\n",
    "- Critical Step to take before modeling\n",
    "- With a balanced target we can be more confident that our modeling results are xx% better than random guessing\n",
    "- With an imbalanced target we are less confident because an imbalanced target throws off the models \n",
    "- We used the **imblearn package** to help balance the target (see code below)  \n",
    "![Code for Balancing the Target](balancing_target.PNG)\n",
    "\n",
    "**Cross Validation vs. Train_Test_Split** \n",
    "- Using the train_test_split, we only have 1 static split of the data for training and testing\n",
    "- Cross Validation is a a better was to split the data for modeling because it gives us k folds for training and testing\n",
    "    - Cross Validation also reduces the \"leaking\" of the testing data into the training data\n",
    "    - We used the **Stratified KFolds** package to do Cross Validation \n",
    "        - Code will be shown below in the modeling section \n",
    "\n",
    "**Feature Engineering**\n",
    "- Based on our discoveries from EDA we decided to do some feature engineering\n",
    "\n",
    "- **X3-X5: Education, Marital Status, Age**\n",
    "    - We created bins for these variables as we saw that they contained values that were not defined in the data dictionary provided\n",
    "    - By binning these variables we made them easier to work with and we have accounted for all the unknown values in the features  \n",
    "![Creating Bins Code Example](edu_bins.PNG)\n",
    "![Education Bins](edu_bins_pic.PNG)\n",
    "![Marital Status Bins](marital_bins_pic.PNG)\n",
    "![Age Bins](age_bins_pic.PNG)\n",
    "  \n",
    "  \n",
    "- **X6-X11: History of Past Payment**\n",
    "    - Only concerned about it the customer paid on time or was late \n",
    "      - Binning makes the feature binary - much easier to work with\n",
    "           - 1 = on time\n",
    "           - 2 = delayed/late\n",
    "        - ![Binning X6-X11](x6bins.PNG)  \n",
    "  \n",
    "  \n",
    "- **X12-X17: Anount of Bill Statement**\n",
    "     - **3 new variables will be created for these features**\n",
    "          - 1: The first will be the absolute value of the original payment amount\n",
    "          - 2: The second will address the positivity or negativity of the original payment amount (binary variable - will account for the negative values in the original that were removed in the first new features - maintains integrity of original data)\n",
    "          - 3: The third will address the time series nature of these features and will be the monthly difference from when the data was collected to the time when the payment was made  \n",
    "  \n",
    "  \n",
    "- **X18-X23: Amount of Previous Payment**\n",
    "     - Only concerned if they paid or not that month - not really the amount they paid \n",
    "          - 0 = no payment\n",
    "          - 1 = payment\n",
    "\n",
    "\n",
    "Notebooks:\n",
    "- [Pipeline 1](pipeline_1/p1_full.ipynb)\n",
    "- [Pipeline 2](pipeline_2/p2_full.ipynb)\n",
    "- [Feature Engineering](comp2_initial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "- The majorit of out time was spent working with various models from basic models like Decision Tree and Logistic Regression to much more advanced models like XGBoost and Random Forest. \n",
    "- These are the models that we worked with during this project:\n",
    "    - Decistion Tree\n",
    "    - Logistic Regression\n",
    "    - SVM\n",
    "    - KNeighborsClassifier\n",
    "    - AdaBoost\n",
    "    - XGBoost\n",
    "    - Random Forest\n",
    "    - TPOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Results/Explanation of the Model with the Best Results that we built**\n",
    "- Explain any \"tweaking\" of the model\n",
    "    - feature selection\n",
    "    - playing with the parameters\n",
    "- Why it may have been the best model\n",
    "- This should be the best model/results we found by following the process from start to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPOT Best Model/Results**\n",
    "- Explain why we also ran a TPOT \n",
    "    - because it optimizes performance rather than reproducibility\n",
    "- Explain the importance of knowing how and when to optimize one or the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
